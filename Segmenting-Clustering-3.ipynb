#!/usr/bin/env python
# coding: utf-8

# # Segmenting and Clustering Neighborhoods in Toronto

# ### Part 1

# ##### Installing LXML to be able to read html page:

# In[1]:


get_ipython().system('pip install lxml')


# ##### Importing required libraries:

# In[2]:


import numpy as np # library to handle data in a vectorized manner
import html5lib
import pandas as pd # library for data analsysis
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
            
import json # library to handle JSON files

get_ipython().system("conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab")
from geopy.geocoders import Nominatim # convert an address into latitude and longitude values

import requests # library to handle requests
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe

# Matplotlib and associated plotting modules
import matplotlib.cm as cm
import matplotlib.colors as colors

# import k-means from clustering stage
from sklearn.cluster import KMeans

get_ipython().system("conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab")
import folium # map rendering library

print('Libraries imported.')


# ##### Reading HTML into 'data' object:

# In[3]:


data = pd.read_html ('https://en.wikipedia.org/w/index.php?title=List_of_postal_codes_of_Canada:_M&direction=prev&oldid=926287641')
#data


# ##### Reading only 1st table of the HTML page into dataframe:

# In[4]:


df = data[0]
df.head()


# ##### Removing the rows with a NAN value in 'Borough' column, also sorted the Dataframe by the column 'Postcode':

# In[5]:


df1 = df[df.Borough != 'Not assigned'].reset_index(drop=True)
df1.sort_values(by='Postcode',ascending=True,inplace=True)
df1.reset_index(drop=True).head(7)


# ##### Identifying the NAN values in the Neighbourhood column.<br>Replacing the NAN values by the corresponding Borough value.

# In[6]:


i=0
for name in list (df1.Neighbourhood):
    if name == 'Not assigned':
        df1.Neighbourhood[i] = df1.Borough[i]
    i+=1
df1.head(7)


# ##### Shrinked the table with the use of Groupby on columns Postcode & Borough. Used count() method to obtain the number of Neighbourhoods in each group.

# In[7]:


dfgroup = df1.groupby(['Postcode','Borough']).count()
dfgroup.reset_index(inplace=True)
dfgroup.head()


# ##### As required, Neighbourhoods with the same Postcode are brought into 1 cell seperated by comma using join function:
# Here, we have used 2 dataframes, df1 & dfgroup. <br>***df1***: It has all the rows obtained from HTML page after removing 'NAN' Boroughs.<br>***dfgroup***: It has unique values of Postcodes
# <br><br>Hence, referring to the Postcode values of dfgroup dataframe, we are traversing throught the df1 dataframe to obtain all the corresponding Neighbourhood values for that particular Postcode. The obtained Neighbourhood values are appended into the list 'lst'. Once, the traversing through the df1 dataframe is complete, the list 'lst' is converted into string seperated by comma and feeded into the corresponding row of Neighbourhood column of the dfgroup dataframe.
# <br>Hence, finally the desired dataframe is obtained.

# In[8]:


j=0
for pcgp in list (dfgroup.Postcode):
    lst = []
    i=0
    for pc1 in list(df1.Postcode):
        if pcgp == pc1:
            lst.append(df1.Neighbourhood[i])
        i+=1
    lst = list (set(lst))
    dfgroup.Neighbourhood[j] = ','.join([str(ngh) for ngh in lst])
    j+=1
    
print ('Loop completed')   
dfgroup


# ##### Shape of dfgroup dataframe:

# In[9]:


dfgroup.shape


# ### Part 2

# ##### Created a new DF named 'dfll' which is a copy of the above dataframe 'dfgroup'. <br>Also, introduced 2 new columns: 'Latitude' and 'Longitude' with a default value of 0.0:

# In[10]:


dfll = dfgroup
dfll.insert(3,'Latitude',0.0,True)
dfll.insert(4,'Longitude',0.0,True)
dfll.head()


# ##### Introduced an 'Address' column that will be passed to the geocoder:

# In[11]:


dfll['Address'] = dfll['Postcode']+','+'Toronto'+','+'Ontario'
dfll.head()


# ##### Importing rate.limiter to introduce delay between the calls to geocoder:

# In[12]:


from geopy.extra.rate_limiter import RateLimiter


# ##### Calling the API to obtain the required location details:

# In[13]:


geolocator = Nominatim(user_agent='abc')
geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)
dfll['location'] = dfll['Address'].apply(geocode)
dfll['point'] = dfll['location'].apply(lambda loc: tuple(loc.point) if loc else None)
dfll[['latitude', 'longitude', 'altitude']] = pd.DataFrame(dfll['point'].tolist(), index=dfll.index)  

dfll.drop(['Latitude','Longitude','point','altitude'],axis=1,inplace=True)


# ##### Below are the obtained values in df format.

# In[14]:


dfll.head(10)


# ##### As the values obtained from geocoder do not match with values in the csv file, below is the data read from the csv file and entered in the dataframe 'dfll':

# In[15]:


import csv
file = 'Geospatial_Coordinates.csv'


# In[16]:


lstgc = []
with open(file,'r') as rfile:
    var = csv.reader(rfile)
    for row in var:
        lstgc.append(row)    
        
lstgc = lstgc[1:]
print (lstgc)


# In[17]:


i = 0
for item in lstgc:
    if item[0] == dfll.Postcode[i]:
        dfll.latitude[i] = item[1]
        dfll.longitude[i] = item[2]
    i+=1
dfll.drop(['Address','location'],axis=1,inplace=True)
dfll.head(10)


# ### Part 3:

# ##### Deriving the co-ordinates of Toronto:

# In[18]:


address = 'Toronto, Ontario'

geolocator = Nominatim(user_agent="ny_explorer")
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto City are {}, {}.'.format(latitude, longitude))


# ##### Marking all the neighbourhoods from the DFLL df onto the map:

# In[19]:


map = folium.Map(location=[latitude, longitude], zoom_start=10)

# add markers to map
for lat, lng, borough, neighborhood in zip(dfll['latitude'], dfll['longitude'], dfll['Borough'], dfll['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map)  
    
map


# ##### Accessing Foursquare API to explore the 2nd entry in the dfll df:

# In[20]:


CLIENT_ID = '04FCLNWPCKIPAC3JYGJ5WXPYIGJF2RIWL2XNLEKQM34I0AQJ' # your Foursquare ID
CLIENT_SECRET = 'FGVB1MEJAWOH0FBR3MI03PBTUSWLOQOBCJ0TI05P4ZYSIWHV' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)


# In[43]:


dfll.loc[1, 'Neighbourhood']


# In[22]:


n_lat = dfll.loc[1, 'latitude'] # neighborhood latitude value
n_lon = dfll.loc[1, 'longitude'] # neighborhood longitude value

n_name = dfll.loc[1, 'Neighbourhood'] # neighborhood name

print('Latitude and longitude values of {} are {}, {}.'.format(n_name, 
                                                               n_lat, 
                                                               n_lon))


# In[23]:


LIMIT = 100 # limit of number of venues returned by Foursquare API

radius = 500 # define radius

# create URL
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    n_lat, 
    n_lon, 
    radius, 
    LIMIT)
url # display URL


# In[24]:


results = requests.get(url).json()
results


# In[25]:


# function that extracts the category of the venue
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']


# In[26]:


venues = results['response']['groups'][0]['items']
#print (venues)    
#norm_venues = json_normalize(venues)
#print ('\n\nnorm_venues: ', norm_venues,'\n\n')
nearby_venues = json_normalize(venues) # flatten JSON

# filter columns
filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns]

# filter the category for each row
nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)

# clean columns
nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns]

nearby_venues.head()


# In[27]:


print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))


# ##### Exploring all the entries from dfll df using a function to access Foursquare API:

# In[28]:


def getNearbyVenues(names, latitudes, longitudes, radius=500):
    
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)


# In[29]:


toronto_venues = getNearbyVenues(names=dfll['Neighbourhood'],
                                   latitudes=dfll['latitude'],
                                   longitudes=dfll['longitude']
                                  )


# In[30]:


print(toronto_venues.shape)
toronto_venues.head()


# In[31]:


toronto_venues.groupby('Neighborhood').count()


# ##### Above table shows the number of venues recieved from the API for the corresponding neighbourhood:

# In[32]:


print('There are {} uniques categories.'.format(len(toronto_venues['Venue Category'].unique())))


# In[33]:


# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.head()


# In[34]:


toronto_onehot.shape


# In[35]:


toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()
toronto_grouped


# In[36]:


toronto_grouped.shape


# ##### The top 5 venues for every neighbourhood and fetch from the above tables and mentioned below:

# In[37]:


num_top_venues = 5

for hood in toronto_grouped['Neighborhood']:
    print("----"+hood+"----")
    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')


# In[38]:


def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]


# In[39]:


num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']

for ind in np.arange(toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()


# ##### Clustering:

# In[40]:


# set number of clusters
kclusters = 5

toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:10] 


# In[41]:


# add clustering labels
#neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = dfll

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighbourhood')

toronto_merged.head() # check the last columns!


# In[44]:


# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(toronto_merged['latitude'], toronto_merged['longitude'], toronto_merged['Neighbourhood'], toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
#        color=rainbow[cluster-1],
        fill=True,
#        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters


# In[ ]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]


# In[ ]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]


# In[ ]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 2, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]


# In[ ]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]


# In[ ]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]


# In[ ]:




